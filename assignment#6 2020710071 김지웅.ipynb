{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.We learned optimizer such as SGD, Adagrad, RMSProp, Adadelta, and Adam.Research the other two state-of-the-art optimizers and explain their feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adamax - Adam의 extension으로 제안된 알고리즘으로 learning rate를 Adam은 L2 norm을 기반으로 조절하지만 AdaMax는 Lp norm으로 확장시킨 알고리즘이다. Adam과 다르게 max operation은 0으로의 bias가 없어서 보정을 해줄 필요가 없다. \n",
    "Nadam - NAG (Nesterov Accelerated Gradient) 방식과 Adam의 개념을 합쳐 NAG에서 사용했던 것 처럼 현재 위치에서 다음 위치로 이동할 기울기와 모멘텀 값을 구하는 것이 아닌 모멘텀 값으로 이동한 뒤 기울기를 구하는 방식. Adam과 NAG의 장점을 합쳐 Adam 보다 좀 더 빠르고 정확하게 global minimum을 찾아낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
