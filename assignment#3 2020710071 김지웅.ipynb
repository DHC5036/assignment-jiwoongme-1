{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Can you design a learnable positional encoding method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어들의 순서에 대한것을 고려하기 위해서 각각의 임베딩에 positional encoding이라는 벡터를 추가\n",
    "이를 통해 모델이 각 단어의 위치와 시퀀스 내의 다른 단어의 위치 차이를 알게해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What can be challenges to Transformers if input sequences are very long?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer가 기존의 RNN방식과 달리 multi-head self attention을 이용해 sequential computation을 줄여 더 많은 단어들 간 dependency를 모델링 한다는 것은 이해했는데,\n",
    "input이 엄청 많아지면 어떻게 될지는 잘 모르겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
